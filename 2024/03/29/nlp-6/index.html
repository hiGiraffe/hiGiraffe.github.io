<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Julian">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://higiraffe.github.io/2024/03/29/nlp-6/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta property="og:type" content="article">
<meta property="og:title" content="【学习模式】自回归模型和GPT">
<meta property="og:url" content="https://higiraffe.github.io/2024/03/29/nlp-6/index.html">
<meta property="og:site_name" content="HiGiraffe">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://higiraffe.github.io/images/nlp-6/1">
<meta property="og:image" content="https://higiraffe.github.io/images/nlp-6/2">
<meta property="og:image" content="https://higiraffe.github.io/images/nlp-6/3">
<meta property="og:image" content="https://higiraffe.github.io/images/nlp-6/4">
<meta property="article:published_time" content="2024-03-29T01:04:05.000Z">
<meta property="article:modified_time" content="2024-03-30T14:21:01.279Z">
<meta property="article:author" content="Julian">
<meta property="article:tag" content="Artificial Intelligence">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://higiraffe.github.io/images/nlp-6/1">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/favicon.ico" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
    <meta name="theme-color" content="#1890ff">
    <link rel="shortcut icon" href="/images/favicon.ico">
    <!--- Page Info-->
    
    <title>
        
            【学习模式】自回归模型和GPT -
        
        HiGiraffe
    </title>
    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
        <link href="home_banner.custom_font.url" rel="stylesheet">
    
    
    
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap" rel="stylesheet">
    
    
        <link href="https://fonts.googleapis.com/css2?family=Inter&display=swap" rel="stylesheet">
    

    <!--- Inject Part-->
    
        
            
    
            
    
            
                
                    <style>.navbar-container .navbar-content .right .desktop .navbar-list .navbar-item, .navbar-container .navbar-content .right .desktop .navbar-list .navbar-item a i, .navbar-container .navbar-content .left .logo-title h1, .navbar-container .navbar-content .right .desktop .navbar-list .navbar-item.search i { color: #808080 !important; } </style>
                
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"higiraffe.github.io","root":"/","language":"en","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":false,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#1890ff","secondary":null},"global":{"fonts":{"chinese":{"enable":true,"family":"Noto Sans SC","url":"https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap"},"english":{"enable":true,"family":"Inter","url":"https://fonts.googleapis.com/css2?family=Inter&display=swap"}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":true},"scroll_progress":{"bar":true,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":false,"site_pv":false,"site_uv":true,"post_pv":false},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/qixingyan.jpg","dark":"/images/qixingyan.jpg"},"title":"A sutdent's learning journey","subtitle":{"text":["做难事必有所得"],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"3rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":true,"family":"Noto Sans SC","url":"home_banner.custom_font.url"},"social_links":{"enable":true,"links":{"github":"https://github.com/hiGiraffe","instagram":null,"zhihu":null,"twitter":null,"email":"hiGiraffe@foxmail.com"},"qrs":null}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.4.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Resources":{"icon":"fa-regular fa-link","path":"/resources/"},"About":{"icon":"fa-regular fa-user","submenus":{"Me":"/about","Github":"https://github.com/hiGiraffe"}}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"Welcome to my homepage!","links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":1}},"footerStart":"2023/10/1 0:0:0"};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="https://higiraffe.github.io/">
                
                HiGiraffe
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/resources/"  >
                                    
                                        
                                            <i class="fa-regular fa-link"></i>
                                        
                                        RESOURCES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        ABOUT&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/about">ME
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/hiGiraffe">GITHUB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer w-full absolute top-0 left-0 bg-background-color">
        <ul class="drawer-navbar-list flex flex-col justify-start items-center">
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/resources/"  >
                             
                                
                                    <i class="fa-regular fa-link"></i>
                                
                                RESOURCES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                ABOUT&nbsp;<i class="group-hover:rotate-180 transition-transform fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" href="/about">ME</a>
                            </li>
                        
                            <li class="text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" target="_blank" rel="noopener" href="https://github.com/hiGiraffe">GITHUB</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                <h1 class="article-title-regular">【学习模式】自回归模型和GPT</h1>
            
            </div>
            
                    
        
        
            <div class="article-header">
                <div class="avatar">
                    <img src="https://avatars.githubusercontent.com/u/146565245?v=4">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Julian</span>
                        
                            <span class="author-label">Lv4</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-03-29 09:04:05</span>
        <span class="mobile">2024-03-29 09:04:05</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-03-30 22:21:01</span>
            <span class="mobile">2024-03-30 22:21:01</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/AI/">AI</a>&nbsp;
                        </li>
                    
                    
                
                    
                        
                            <li>></li>
                        
                        <li>
                            <a href="/categories/AI/LLM/">LLM</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Artificial-Intelligence/">Artificial Intelligence</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body">
            <p><a class="link" target="_blank" rel="noopener" href="https://medium.com/@sntaus/understanding-self-attention-gpt-models-80ec894eebf0">原文链接 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h3><ol>
<li>词嵌入。将词变成向量，句子变成向量集合。</li>
<li>通过数学方法将位置编码嵌入到词向量中。</li>
</ol>
<h3 id="Query-Key-Value"><a href="#Query-Key-Value" class="headerlink" title="Query, Key, Value"></a>Query, Key, Value</h3><blockquote>
<ol>
<li><p>Use the vector to create a <em>query vector</em> for that word in that position — this just means the input position encoded vector goes through a mathematical function (whose parameters are trained during training) to produce a new vector that is referred to as the <em>query</em> vector.</p>
<p>输入向量通过数学方式生成query向量</p>
</li>
<li><p>Use the vector to create a <em>key vector</em> for that word in that position —i.e. the input position encoded vector goes through <em>another</em> (different) mathematical function to produce another new vector that is referred to as the <em>key</em> vector.</p>
<p>输入向量通过另一种数学方式生成key向量</p>
</li>
<li><p>Use the vector to create a <em>value vector</em> for that word in that position — i.e. the vector goes through <em>yet another</em> mathematical function to produce yet another new vector that is referred to as the <em>value</em> vector.</p>
<p>输入向量通过另一种数学方式生成value向量</p>
</li>
</ol>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/nlp-6/1" alt="img"></p>
<p>数学方法主要是权重矩阵</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/nlp-6/2" alt="img"></p>
<p>并在后续使用softmax保证分数没太大区别。</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/nlp-6/3" alt="img"></p>
</blockquote>
<h3 id="进一步流程"><a href="#进一步流程" class="headerlink" title="进一步流程"></a>进一步流程</h3><blockquote>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/nlp-6/4" alt="img"></p>
</blockquote>
<h2 id="How-does-all-of-it-happen-so-fast-so-many-times"><a href="#How-does-all-of-it-happen-so-fast-so-many-times" class="headerlink" title="How does all of it happen so fast so many times?"></a>How does all of it happen so fast so many times?</h2><blockquote>
<p>First, the model does not have to recompute query, key, value vectors each time for the words preceding the last output word since it already did it in previous stages and can re-use those values.</p>
<p>不需要每次都计算KQV</p>
<p>Secondly, most of the steps in the self-attention layer can be parallelized.</p>
<p>自注意力层可以并行</p>
<p>Lastly, there’s likely various optimizations both within the network like pruning and at the hardware level that can be leveraged to speed up inference time even more.</p>
<p>可能会有修剪或各方面的硬件优化</p>
</blockquote>
<h2 id="how-does-the-model-learn"><a href="#how-does-the-model-learn" class="headerlink" title="how does the model learn?"></a>how does the model learn?</h2><p>采用反向传播梯度下降</p>
<blockquote>
<ol>
<li><p>The model is given an input sequence and it predicts the next word using self-attention.</p>
<p>根据自注意力预测下一个单词</p>
</li>
<li><p>If the predicted next word is not the same as the expected next word, the model parameters are updated using gradient descent.</p>
<p>跟真实情况有差距的话就使用梯度下降更新模型参数</p>
</li>
<li><p>This process is repeated for numerous times using a large training dataset (with labelled input and expected output).</p>
<p>使用大型训练数据集不断重复该过程</p>
</li>
</ol>
</blockquote>
<p>且这种大模型与transformer不同的话是其仅使用decoder。</p>
<h3 id="代码细节"><a href="#代码细节" class="headerlink" title="代码细节"></a>代码细节</h3><p><a class="link" target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/blob/v0.2.7/vllm/model_executor/models/llama.py">代码地址 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<hr>
<p>LlamaForCausalLM开始forward，调用LlamaModel的forward函数，返回隐藏层输出状态。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaForCausalLM</span>(nn.Module):</span><br><span class="line">    <span class="comment">#通过LlamaModel得到输出状态</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_ids: torch.Tensor,</span></span><br><span class="line"><span class="params">        positions: torch.Tensor,</span></span><br><span class="line"><span class="params">        kv_caches: <span class="type">List</span>[KVCache],</span></span><br><span class="line"><span class="params">        input_metadata: InputMetadata,</span></span><br><span class="line"><span class="params">    </span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.model(input_ids, positions, kv_caches,</span><br><span class="line">                                   input_metadata)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure></div>


<hr>
<p>这里先介绍RmsNorm，均方根归一化，后文多次用到</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Root mean square normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Computes x -&gt; w * x / sqrt(E[x^2] + eps) where w is the learned weight.</span></span><br><span class="line"><span class="string">    Refer to https://arxiv.org/abs/1910.07467</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        x: torch.Tensor,</span></span><br><span class="line"><span class="params">        residual: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Union</span>[torch.Tensor, <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]]:</span><br><span class="line">        <span class="comment"># 如果有残差向量，调用加速，进行计算</span></span><br><span class="line">        <span class="keyword">if</span> residual <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            ops.fused_add_rms_norm(</span><br><span class="line">                x,</span><br><span class="line">                residual,</span><br><span class="line">                self.weight.data,</span><br><span class="line">                self.variance_epsilon,</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">return</span> x, residual</span><br><span class="line">        <span class="comment"># 如果没有残差向量，创建一个和x一样的out进行计算</span></span><br><span class="line">        out = torch.empty_like(x)</span><br><span class="line">        ops.rms_norm(</span><br><span class="line">            out,</span><br><span class="line">            x,</span><br><span class="line">            self.weight.data,</span><br><span class="line">            self.variance_epsilon,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></div>


<hr>
<p>LlamaModel forward先进行词嵌入，再循环进行N次decoder操作（调用LlamaDecoderLayer的forward）</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_ids: torch.Tensor,</span></span><br><span class="line"><span class="params">        positions: torch.Tensor,</span></span><br><span class="line"><span class="params">        kv_caches: <span class="type">List</span>[KVCache],</span></span><br><span class="line"><span class="params">        input_metadata: InputMetadata,</span></span><br><span class="line"><span class="params">    </span>) -&gt; torch.Tensor:</span><br><span class="line">        hidden_states = self.embed_tokens(input_ids)	<span class="comment">#词嵌入</span></span><br><span class="line">        residual = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.layers)):	<span class="comment">#循环每一层，每次进行一个decoder处理</span></span><br><span class="line">            layer = self.layers[i]			</span><br><span class="line">            hidden_states, residual = layer(</span><br><span class="line">                positions,</span><br><span class="line">                hidden_states,</span><br><span class="line">                kv_caches[i],</span><br><span class="line">                input_metadata,</span><br><span class="line">                residual,</span><br><span class="line">            )</span><br><span class="line">        hidden_states, _ = self.norm(hidden_states, residual) <span class="comment">#RMSNorm归一化返回一个状态</span></span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure></div>
<hr>
<p>LlamaDecoderLayer</p>
<p>先根据残差向量计算（调用ResNorm实现归一化）</p>
<p>然后调用注意力机制的forward计算attention</p>
<p>然后调用ResNorm再进行归一化</p>
<p>最后调用mlp线性层</p>
<p><strong>那么，</strong></p>
<p><strong>1. 前面hidden_states 对应的residual计算部分对应的内容可以在哪里查阅到？</strong></p>
<p><strong>2. 我的理解是这里跟resnet相似，那不需要最后add残差向量吗？</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaDecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        positions: torch.Tensor,</span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor,</span></span><br><span class="line"><span class="params">        kv_cache: KVCache,</span></span><br><span class="line"><span class="params">        input_metadata: InputMetadata,</span></span><br><span class="line"><span class="params">        residual: <span class="type">Optional</span>[torch.Tensor],</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line">        <span class="comment"># Self Attention</span></span><br><span class="line">        <span class="comment"># 根据残差向量计算隐藏层</span></span><br><span class="line">        <span class="keyword">if</span> residual <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            residual = hidden_states</span><br><span class="line">            hidden_states = self.input_layernorm(hidden_states) <span class="comment"># ResNorm</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hidden_states, residual = self.input_layernorm( <span class="comment"># ResNorm</span></span><br><span class="line">                hidden_states, residual)</span><br><span class="line">        <span class="comment"># 计算attention</span></span><br><span class="line">        hidden_states = self.self_attn(</span><br><span class="line">            positions=positions,</span><br><span class="line">            hidden_states=hidden_states,</span><br><span class="line">            kv_cache=kv_cache,</span><br><span class="line">            input_metadata=input_metadata,</span><br><span class="line">        )</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># Fully Connected</span></span><br><span class="line">        hidden_states, residual = self.post_attention_layernorm( <span class="comment"># ResNorm</span></span><br><span class="line">            hidden_states, residual)</span><br><span class="line">        hidden_states = self.mlp(hidden_states) <span class="comment"># mlp线性层</span></span><br><span class="line">        <span class="keyword">return</span> hidden_states, residual</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>

<hr>
<p>LlamaAttention先调用了QKVParallelLinear来<strong>计算 ___？</strong>gpt说是将隐藏状态对K、Q、V投影</p>
<p>再调用了qkv.split，<strong>这是哪里的？</strong>gpt说是将投影的张量分割成K、Q、V</p>
<p>再调用rotary_emb，是另外一个文件中的get_rope。<strong>不知道干什么的。</strong>gpt说使用 <code>rotary_emb</code> 层对查询和键进行旋转位置编码。</p>
<p>调用PageAttention计算attention</p>
<p>最后调用RowParallelLinear来计算结果，<strong>干什么的？</strong>gpt说通过 <code>o_proj</code> 对注意力计算的输出进行投影，得到最终的输出。</p>
<p><strong>投影是指？</strong></p>
<p>总而言之，这里就是attention的计算部分，<strong>需要后面找文献和公式对应一下</strong></p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaAttention</span>(nn.Module):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        positions: torch.Tensor, <span class="comment"># 位置张量</span></span></span><br><span class="line"><span class="params">        hidden_states: torch.Tensor, <span class="comment"># 隐藏状态张量</span></span></span><br><span class="line"><span class="params">        kv_cache: KVCache, <span class="comment"># 键值缓存，主要是vllm的一个特色，用来存QKV的</span></span></span><br><span class="line"><span class="params">        input_metadata: InputMetadata, <span class="comment">#输入的元数据信息</span></span></span><br><span class="line"><span class="params">    </span>) -&gt; torch.Tensor:</span><br><span class="line">        qkv, _ = self.qkv_proj(hidden_states) <span class="comment"># 分块计算</span></span><br><span class="line">        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-<span class="number">1</span>)</span><br><span class="line">        q, k = self.rotary_emb(positions, q, k)</span><br><span class="line">        k_cache, v_cache = kv_cache</span><br><span class="line">        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata) <span class="comment"># pageAttention计算</span></span><br><span class="line">        output, _ = self.o_proj(attn_output) <span class="comment"># 分块计算</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></div>

<hr>
<p>LlamaMLP主要就是一个MLP部分</p>
<p>先将输入参数x进行投影，得到gate_up</p>
<p>中间层输出经过act_fn激活</p>
<p>最后将激活后的结果进行投影</p>
<p>得到输出</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        gate_up, _ = self.gate_up_proj(x)</span><br><span class="line">        x = self.act_fn(gate_up)</span><br><span class="line">        x, _ = self.down_proj(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></div>




        </div>

        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Artificial-Intelligence/">#Artificial Intelligence</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                        rel="prev"
                        href="/2024/03/30/llm-4/"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">【学习笔记】vLLM</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/2024/03/28/llm-3/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">llama2部署记录</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">【学习模式】自回归模型和GPT</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="nav-text">基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="nav-text">输入数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Query-Key-Value"><span class="nav-text">Query, Key, Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%B5%81%E7%A8%8B"><span class="nav-text">进一步流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-does-all-of-it-happen-so-fast-so-many-times"><span class="nav-text">How does all of it happen so fast so many times?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#how-does-the-model-learn"><span class="nav-text">how does the model learn?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%BB%86%E8%8A%82"><span class="nav-text">代码细节</span></a></li></ol></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2023</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Julian</a>
        </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.4.4</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
            
                
        
                
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex justify-center items-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
        ],
        containers: ["#swup"],
    });

    swup.hooks.on("page:view", () => {
        Global.refresh();
    });

    // if (document.readyState === "complete") {
    //
    // } else {
    //     document.addEventListener("DOMContentLoaded", () => init());
    // }
</script>






<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/libs/anime.min.js"></script>

        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
