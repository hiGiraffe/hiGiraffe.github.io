<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Julian">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://higiraffe.github.io/2024/03/28/llm-3/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta property="og:type" content="article">
<meta property="og:title" content="llama2部署记录">
<meta property="og:url" content="https://higiraffe.github.io/2024/03/28/llm-3/index.html">
<meta property="og:site_name" content="HiGiraffe">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-03-28T11:02:45.000Z">
<meta property="article:modified_time" content="2024-04-05T16:23:43.942Z">
<meta property="article:author" content="Julian">
<meta property="article:tag" content="Artificial Intelligence">
<meta name="twitter:card" content="summary">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/favicon.ico" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
    <meta name="theme-color" content="#1890ff">
    <link rel="shortcut icon" href="/images/favicon.ico">
    <!--- Page Info-->
    
    <title>
        
            llama2部署记录 -
        
        HiGiraffe
    </title>
    
<link rel="stylesheet" href="/css/style.css">


    
        
<link rel="stylesheet" href="/css/build/styles.css">

    

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
        <link href="home_banner.custom_font.url" rel="stylesheet">
    
    
    
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap" rel="stylesheet">
    
    
        <link href="https://fonts.googleapis.com/css2?family=Inter&display=swap" rel="stylesheet">
    

    <!--- Inject Part-->
    
        
            
    
            
    
            
                
                    <style>.navbar-container .navbar-content .right .desktop .navbar-list .navbar-item, .navbar-container .navbar-content .right .desktop .navbar-list .navbar-item a i, .navbar-container .navbar-content .left .logo-title h1, .navbar-container .navbar-content .right .desktop .navbar-list .navbar-item.search i { color: #808080 !important; } </style>
                
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"higiraffe.github.io","root":"/","language":"en","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":false,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":false,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#1890ff","secondary":null},"global":{"fonts":{"chinese":{"enable":true,"family":"Noto Sans SC","url":"https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap"},"english":{"enable":true,"family":"Inter","url":"https://fonts.googleapis.com/css2?family=Inter&display=swap"}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":true},"scroll_progress":{"bar":true,"percentage":true},"website_counter":{"url":"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js","enable":false,"site_pv":false,"site_uv":false,"post_pv":false},"single_page":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/qixingyan.jpg","dark":"/images/qixingyan.jpg"},"title":"A sutdent's learning journey","subtitle":{"text":["... and occasional confusion"],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#d9d9d9","dark":"#bcbcbc"},"text_style":{"title_size":"3rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":true,"family":"Noto Sans SC","url":"home_banner.custom_font.url"},"social_links":{"enable":true,"links":{"github":"https://github.com/hiGiraffe","instagram":null,"zhihu":null,"twitter":null,"email":"hiGiraffe@foxmail.com"},"qrs":null}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.4.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Resources":{"icon":"fa-regular fa-link","path":"/resources/"},"About":{"icon":"fa-regular fa-user","submenus":{"Me":"/about","Github":"https://github.com/hiGiraffe"}}},"search":{"enable":true,"preload":true}},"page_templates":{"friends_column":3,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":"That's great! Just remember, if your study notes start talking back to you, it might be time for a break!","links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":1}},"footerStart":"2023/10/1 0:0:0"};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="swup-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container" id="swup">

    

    <div class="main-content-container">


        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="https://higiraffe.github.io/">
                
                HiGiraffe
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        ARCHIVES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/resources/"  >
                                    
                                        
                                            <i class="fa-regular fa-link"></i>
                                        
                                        RESOURCES
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        ABOUT&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/about">ME
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/hiGiraffe">GITHUB
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer w-full absolute top-0 left-0 bg-background-color">
        <ul class="drawer-navbar-list flex flex-col justify-start items-center">
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                ARCHIVES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group " 
                        href="/resources/"  >
                             
                                
                                    <i class="fa-regular fa-link"></i>
                                
                                RESOURCES
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item text-base my-1.5 flex justify-center items-center">
                        <a class="rounded-3xl py-1.5 px-5 hover:border hover:!text-primary active:!text-primary group has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                ABOUT&nbsp;<i class="group-hover:rotate-180 transition-transform fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" href="/about">ME</a>
                            </li>
                        
                            <li class="text-base flex justify-center items-center hover:underline active:underline hover:underline-offset-1 rounded-3xl">
                                <a class="py-0.5" target="_blank" rel="noopener" href="https://github.com/hiGiraffe">GITHUB</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="post-page-container">
    <div class="article-content-container">

        <div class="article-title">
            
                <h1 class="article-title-regular">llama2部署记录</h1>
            
            </div>
            
                    
        
        
            <div class="article-header">
                <div class="avatar">
                    <img src="https://avatars.githubusercontent.com/u/146565245?v=4">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">Julian</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-03-28 19:02:45</span>
        <span class="mobile">2024-03-28 19:02:45</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-04-06 00:23:43</span>
            <span class="mobile">2024-04-06 00:23:43</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/AI/">AI</a>&nbsp;
                        </li>
                    
                    
                
                    
                        
                            <li>></li>
                        
                        <li>
                            <a href="/categories/AI/LLM/">LLM</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Artificial-Intelligence/">Artificial Intelligence</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                    </div>
                </div>
            </div>
        

        


        <div class="article-content markdown-body">
            <a class="button  center regular" target="_blank" rel="noopener" href="https://llama.meta.com/llama2/" title="Llama 2: open source, free for research and commercial use"><i class="fa-solid fa-paperclip"></i> Llama 2: open source, free for research and commercial use</a>

<a class="button  center regular" target="_blank" rel="noopener" href="https://huggingface.co/meta-llama" title="Llama 2 in Hugging Face"><i class="fa-solid fa-paperclip"></i> Llama 2 in Hugging Face</a>

<a class="button  center regular" target="_blank" rel="noopener" href="https://github.com/meta-llama/llama" title="Llama 2 in Github"><i class="fa-solid fa-paperclip"></i> Llama 2 in Github</a>

<p>配置好CUDA、Pytorch，下载模型数据</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install -e .</span><br><span class="line">torchrun --nproc_per_node 1 example_text_completion.py \</span><br><span class="line">    --ckpt_dir llama-2-7b/ \</span><br><span class="line">    --tokenizer_path tokenizer.model \</span><br><span class="line">    --max_seq_len 128 --max_batch_size 4</span><br></pre></td></tr></table></figure></div>


<h2 id="VLLM部署记录"><a href="#VLLM部署记录" class="headerlink" title="VLLM部署记录"></a>VLLM部署记录</h2><p><strong>报错</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: /home/cjl/llama/llama-2-7b does not appear to have a file named config.json.</span><br></pre></td></tr></table></figure></div>

<p>谷歌搜索到是因为weight需要是hf格式，需要利用transformer提供的convert_llama_weights_to_hf.py脚本将其变为hf格式。</p>
<p>参考<a class="link" target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/llama2">transformers llama2 hugging face文档 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br><span class="line">    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</span><br></pre></td></tr></table></figure></div>

<p>其中在本机上transformers位置</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/cjl/anaconda3/envs/vllm/lib/python3.9/site-packages/transformers/</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python /home/cjl/anaconda3/envs/vllm/lib/python3.9/site-packages/transformers/models/llama/convert_llama_weights_to_hf.py  \</span><br><span class="line">    --input_dir /home/cjl/llama/llama-2-7b --model_size 7B --output_dir /home/cjl/llama/llama-2-7b-hf</span><br></pre></td></tr></table></figure></div>

<p><strong>仍然报错</strong></p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: [enforce fail at inline_container.cc:424] </span><br></pre></td></tr></table></figure></div>

<p>谷歌没有搜到答案，后经验证确定，是由于硬盘空间不够。。</p>
<p>清理空间后，成功解决。</p>
<p>创建vllm_demo.py文件并运行，一个简单的demo就实现了。</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    <span class="string">"Hello, my name is"</span>,</span><br><span class="line">    <span class="string">"The president of the United States is"</span>,</span><br><span class="line">    <span class="string">"The capital of France is"</span>,</span><br><span class="line">    <span class="string">"The future of AI is"</span>,</span><br><span class="line">]</span><br><span class="line">sampling_params = SamplingParams(temperature=<span class="number">0.8</span>, top_p=<span class="number">0.95</span>)</span><br><span class="line">llm = LLM(model=<span class="string">'/home/cjl/llama/llama-2-7b-hf'</span>, dtype=<span class="string">'half'</span>) </span><br><span class="line"></span><br><span class="line">outputs = llm.generate(prompts, sampling_params)</span><br><span class="line"><span class="comment"># Print the outputs.</span></span><br><span class="line"><span class="keyword">for</span> output <span class="keyword">in</span> outputs:</span><br><span class="line">    prompt = output.prompt</span><br><span class="line">    generated_text = output.outputs[<span class="number">0</span>].text</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Prompt: <span class="subst">{prompt!r}</span>, Generated text: <span class="subst">{generated_text!r}</span>"</span>)</span><br></pre></td></tr></table></figure></div>

<h3 id="chat-completion功能"><a href="#chat-completion功能" class="headerlink" title="chat completion功能"></a>chat completion功能</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{</span><br><span class="line">        "model": "/home/cjl/llama/llama-2-7b-hf",</span><br><span class="line">        "messages": [</span><br><span class="line">            {"role": "system", "content": "You are an intelligent British female writer and translator who is good at writing science fiction using multiple languages. You won a Nobel price in literature five years ago."},</span><br><span class="line">            {"role": "user", "content": "Please detailedly tell a story about an exciting aerospace expedition for a Chinese boy Lam and his German dog. They are sent to aerospace by mistake and strive to wait for rescue from motherland with no water and food supply for over a month. They are almost caught by aliens disguised as his mother. Moreover, please translate the above story to Chinese, German, French, Portuguese and Japanese respectively."}</span><br><span class="line">        ], "temperature": 0</span><br><span class="line">    }'</span><br></pre></td></tr></table></figure></div>



<h2 id="Accelerate部署记录"><a href="#Accelerate部署记录" class="headerlink" title="Accelerate部署记录"></a>Accelerate部署记录</h2><a class="button  center regular" target="_blank" rel="noopener" href="https://huggingface.co/docs/accelerate/en/index" title="Accelerate in Hugging Face"><i class="fa-solid fa-paperclip"></i> Accelerate in Hugging Face</a>

<a class="button  center regular" target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/news/1257333" title="Accelerate单机多卡简单demo"><i class="fa-solid fa-paperclip"></i> Accelerate单机多卡简单demo</a>



<p>第一次使用一些优化设置，但貌似硬件不太适配，所以第二次设置了基本什么优化都没有的情况。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> `Accelerate` version: 0.29.0.dev0</span><br><span class="line">- Platform: Linux-5.14.0-362.13.1.el9_3.x86_64-x86_64-with-glibc2.34</span><br><span class="line">- Python version: 3.10.14</span><br><span class="line">- Numpy version: 1.26.4</span><br><span class="line">- PyTorch version (GPU?): 2.2.1+cu121 (True)</span><br><span class="line">- PyTorch XPU available: False</span><br><span class="line">- PyTorch NPU available: False</span><br><span class="line">- PyTorch MLU available: False</span><br><span class="line">- System RAM: 187.06 GB</span><br><span class="line">- GPU type: Tesla V100S-PCIE-32GB</span><br><span class="line">- `Accelerate` default config:</span><br><span class="line">        - compute_environment: LOCAL_MACHINE</span><br><span class="line">        - distributed_type: MULTI_GPU</span><br><span class="line">        - mixed_precision: no</span><br><span class="line">        - use_cpu: False</span><br><span class="line">        - debug: True</span><br><span class="line">        - num_processes: 2</span><br><span class="line">        - machine_rank: 0</span><br><span class="line">        - num_machines: 1</span><br><span class="line">        - gpu_ids: all</span><br><span class="line">        - rdzv_backend: static</span><br><span class="line">        - same_network: True</span><br><span class="line">        - main_training_function: main</span><br><span class="line">        - enable_cpu_affinity: False</span><br><span class="line">        - downcast_bf16: no</span><br><span class="line">        - tpu_use_cluster: False</span><br><span class="line">        - tpu_use_sudo: False</span><br><span class="line">        - tpu_env: []</span><br></pre></td></tr></table></figure></div>

<h2 id="demo-1-简单使用"><a href="#demo-1-简单使用" class="headerlink" title="demo 1 简单使用"></a>demo 1 简单使用</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> accelerate.utils <span class="keyword">import</span> gather_object</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># each GPU creates a string</span></span><br><span class="line"></span><br><span class="line">message=[ <span class="string">f"Hello this is GPU <span class="subst">{accelerator.process_index}</span>"</span> ] </span><br><span class="line"></span><br><span class="line"><span class="comment"># collect the messages from all GPUs</span></span><br><span class="line"></span><br><span class="line">messages=gather_object(message)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output the messages only on the main process with accelerator.print() </span></span><br><span class="line"></span><br><span class="line">accelerator.<span class="built_in">print</span>(messages)</span><br></pre></td></tr></table></figure></div>

<p>需要注意我们要采用accelerate运行，而不是python运行。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ accelerate launch acc_demo_1.py </span><br><span class="line">['Hello this is GPU 0', 'Hello this is GPU 1']</span><br></pre></td></tr></table></figure></div>
<h2 id="llm-demo"><a href="#llm-demo" class="headerlink" title="llm demo"></a>llm demo</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> accelerate.utils <span class="keyword">import</span> gather_object</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> statistics <span class="keyword">import</span> mean</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch, time, json</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books</span></span><br><span class="line"></span><br><span class="line">prompts_all=[</span><br><span class="line"></span><br><span class="line">    <span class="string">"The King is dead. Long live the Queen."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"Once there were four children whose names were Peter, Susan, Edmund, and Lucy."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"The story so far: in the beginning, the universe was created."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"It was a bright cold day in April, and the clocks were striking thirteen."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"The sweat wis lashing oafay Sick Boy; he wis trembling."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"124 was spiteful. Full of Baby's venom."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"I write this sitting in the kitchen sink."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold."</span>,</span><br><span class="line"></span><br><span class="line">] * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load a base model and tokenizer</span></span><br><span class="line"></span><br><span class="line">model_path=<span class="string">"/home/cjl/llama/llama-2-7b-hf"</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line"></span><br><span class="line">    model_path,   </span><br><span class="line"></span><br><span class="line">    device_map={<span class="string">""</span>: accelerator.process_index},</span><br><span class="line"></span><br><span class="line">    torch_dtype=torch.bfloat16,</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)   </span><br><span class="line"></span><br><span class="line"><span class="comment"># sync GPUs and start the timer</span></span><br><span class="line"></span><br><span class="line">accelerator.wait_for_everyone()</span><br><span class="line"></span><br><span class="line">start=time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># divide the prompt list onto the available GPUs </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> accelerator.split_between_processes(prompts_all) <span class="keyword">as</span> prompts:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store output of generations in dict</span></span><br><span class="line"></span><br><span class="line">    results=<span class="built_in">dict</span>(outputs=[], num_tokens=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># have each GPU do inference, prompt by prompt</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> prompt <span class="keyword">in</span> prompts:</span><br><span class="line"></span><br><span class="line">        prompt_tokenized=tokenizer(prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda"</span>)</span><br><span class="line"></span><br><span class="line">        output_tokenized = model.generate(**prompt_tokenized, max_new_tokens=<span class="number">100</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># remove prompt from output </span></span><br><span class="line"></span><br><span class="line">        output_tokenized=output_tokenized[<span class="built_in">len</span>(prompt_tokenized[<span class="string">"input_ids"</span>][<span class="number">0</span>]):]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># store outputs and number of tokens in result{}</span></span><br><span class="line"></span><br><span class="line">        results[<span class="string">"outputs"</span>].append( tokenizer.decode(output_tokenized) )</span><br><span class="line"></span><br><span class="line">        results[<span class="string">"num_tokens"</span>] += <span class="built_in">len</span>(output_tokenized)</span><br><span class="line"></span><br><span class="line">    results=[ results ] <span class="comment"># transform to list, otherwise gather_object() will not collect correctly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># collect results from all the GPUs</span></span><br><span class="line"></span><br><span class="line">results_gathered=gather_object(results)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line"></span><br><span class="line">    timediff=time.time()-start</span><br><span class="line"></span><br><span class="line">    num_tokens=<span class="built_in">sum</span>([r[<span class="string">"num_tokens"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results_gathered ])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"tokens/sec: <span class="subst">{num_tokens//timediff}</span>, time <span class="subst">{timediff}</span>, total tokens <span class="subst">{num_tokens}</span>, total prompts <span class="subst">{<span class="built_in">len</span>(prompts_all)}</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(results)</span><br></pre></td></tr></table></figure></div>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokens/sec: 59.0, time 168.46036076545715, total tokens 10000, total prompts 100</span><br></pre></td></tr></table></figure></div>

<h2 id="llm-批处理-demo"><a href="#llm-批处理-demo" class="headerlink" title="llm 批处理 demo"></a>llm 批处理 demo</h2><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> accelerate.utils <span class="keyword">import</span> gather_object</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> statistics <span class="keyword">import</span> mean</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch, time, json</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">write_pretty_json</span>(<span class="params">file_path, data</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"w"</span>) <span class="keyword">as</span> write_file:</span><br><span class="line"></span><br><span class="line">        json.dump(data, write_file, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10*10 Prompts. Source: https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books</span></span><br><span class="line"></span><br><span class="line">prompts_all=[</span><br><span class="line"></span><br><span class="line">    <span class="string">"The King is dead. Long live the Queen."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"Once there were four children whose names were Peter, Susan, Edmund, and Lucy."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"The story so far: in the beginning, the universe was created."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"It was a bright cold day in April, and the clocks were striking thirteen."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"The sweat wis lashing oafay Sick Boy; he wis trembling."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"124 was spiteful. Full of Baby's venom."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"I write this sitting in the kitchen sink."</span>,</span><br><span class="line"></span><br><span class="line">    <span class="string">"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold."</span>,</span><br><span class="line"></span><br><span class="line">] * <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load a base model and tokenizer</span></span><br><span class="line"></span><br><span class="line">model_path=<span class="string">"models/llama2-7b"</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line"></span><br><span class="line">    model_path,   </span><br><span class="line"></span><br><span class="line">    device_map={<span class="string">""</span>: accelerator.process_index},</span><br><span class="line"></span><br><span class="line">    torch_dtype=torch.bfloat16,</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)   </span><br><span class="line"></span><br><span class="line">tokenizer.pad_token = tokenizer.eos_token</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch, left pad (for inference), and tokenize</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_prompts</span>(<span class="params">prompts, tokenizer, batch_size=<span class="number">16</span></span>):</span><br><span class="line"></span><br><span class="line">    batches=[prompts[i:i + batch_size] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(prompts), batch_size)] </span><br><span class="line"></span><br><span class="line">    batches_tok=[]</span><br><span class="line"></span><br><span class="line">    tokenizer.padding_side=<span class="string">"left"</span>     </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> prompt_batch <span class="keyword">in</span> batches:</span><br><span class="line"></span><br><span class="line">        batches_tok.append(</span><br><span class="line"></span><br><span class="line">            tokenizer(</span><br><span class="line"></span><br><span class="line">                prompt_batch, </span><br><span class="line"></span><br><span class="line">                return_tensors=<span class="string">"pt"</span>, </span><br><span class="line"></span><br><span class="line">                padding=<span class="string">'longest'</span>, </span><br><span class="line"></span><br><span class="line">                truncation=<span class="literal">False</span>, </span><br><span class="line"></span><br><span class="line">                pad_to_multiple_of=<span class="number">2</span>,</span><br><span class="line"></span><br><span class="line">                add_special_tokens=<span class="literal">False</span>).to(<span class="string">"cuda"</span>) </span><br><span class="line"></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    tokenizer.padding_side=<span class="string">"right"</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batches_tok</span><br><span class="line"></span><br><span class="line"><span class="comment"># sync GPUs and start the timer</span></span><br><span class="line"></span><br><span class="line">accelerator.wait_for_everyone()   </span><br><span class="line"></span><br><span class="line">start=time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># divide the prompt list onto the available GPUs </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> accelerator.split_between_processes(prompts_all) <span class="keyword">as</span> prompts:</span><br><span class="line"></span><br><span class="line">    results=<span class="built_in">dict</span>(outputs=[], num_tokens=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># have each GPU do inference in batches</span></span><br><span class="line"></span><br><span class="line">    prompt_batches=prepare_prompts(prompts, tokenizer, batch_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> prompts_tokenized <span class="keyword">in</span> prompt_batches:</span><br><span class="line"></span><br><span class="line">        outputs_tokenized=model.generate(**prompts_tokenized, max_new_tokens=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># remove prompt from gen. tokens</span></span><br><span class="line"></span><br><span class="line">        outputs_tokenized=[ tok_out[<span class="built_in">len</span>(tok_in):] </span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> tok_in, tok_out <span class="keyword">in</span> <span class="built_in">zip</span>(prompts_tokenized[<span class="string">"input_ids"</span>], outputs_tokenized) ] </span><br><span class="line"></span><br><span class="line">        <span class="comment"># count and decode gen. tokens </span></span><br><span class="line"></span><br><span class="line">        num_tokens=<span class="built_in">sum</span>([ <span class="built_in">len</span>(t) <span class="keyword">for</span> t <span class="keyword">in</span> outputs_tokenized ])</span><br><span class="line"></span><br><span class="line">        outputs=tokenizer.batch_decode(outputs_tokenized)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># store in results{} to be gathered by accelerate</span></span><br><span class="line"></span><br><span class="line">        results[<span class="string">"outputs"</span>].extend(outputs)</span><br><span class="line"></span><br><span class="line">        results[<span class="string">"num_tokens"</span>] += num_tokens</span><br><span class="line"></span><br><span class="line">    results=[ results ] <span class="comment"># transform to list, otherwise gather_object() will not collect correctly</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># collect results from all the GPUs</span></span><br><span class="line"></span><br><span class="line">results_gathered=gather_object(results)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line"></span><br><span class="line">    timediff=time.time()-start</span><br><span class="line"></span><br><span class="line">    num_tokens=<span class="built_in">sum</span>([r[<span class="string">"num_tokens"</span>] <span class="keyword">for</span> r <span class="keyword">in</span> results_gathered ])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"tokens/sec: <span class="subst">{num_tokens//timediff}</span>, time elapsed: <span class="subst">{timediff}</span>, num_tokens <span class="subst">{num_tokens}</span>"</span>)</span><br></pre></td></tr></table></figure></div>

<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokens/sec: 113.0, time elapsed: 87.8477463722229, num_tokens 10000</span><br></pre></td></tr></table></figure></div>



<a class="button  center regular" target="_blank" rel="noopener" href="https://huggingface.co/docs/accelerate/main/en/concept_guides/big_model_inference" title="Accelerate Handling big models for inference"><i class="fa-solid fa-paperclip"></i> Accelerate Handling big models for inference</a>

<p>修改device_map实现layer分层</p>
<p>修改前</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line"></span><br><span class="line">    model_path,   </span><br><span class="line"></span><br><span class="line">    device_map={<span class="string">""</span>: accelerator.process_index},</span><br><span class="line"></span><br><span class="line">    torch_dtype=torch.bfloat16,</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:08&lt;00:00, 22.69s/it]</span><br><span class="line">You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers</span><br><span class="line">Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:10&lt;00:00, 23.42s/it]</span><br><span class="line">You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">tokens/sec: 25.0, time 38.98043131828308, total tokens 1000, total prompts 10</span><br><span class="line">device_map is  {'': 0}</span><br><span class="line">[{'outputs': ['\nThe King is dead. Long live the Queen.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\n', 'They were sent to the country to stay with their eccentric uncle, who lived in a large house that had been in his family for hundreds of years.\nTheir uncle was a very strange man. He was tall and thin and had a long, hooked nose. He was always dressed in a long black cloak, and he had a long, white beard. He was very fond of children, and he was always telling them stories about the witches who lived in the', 'The universe was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. And God said, "Let there be light," and there was light. And God saw the light, that it was good; and God divided the light from the darkness. And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\nAnd God said, "Let there', '\nWinston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a colored poster, too large for indoor display, had been tacked to the wall. It', '\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\n"It is a truth universally acknowledged, that a single man in possession of a good'], 'num_tokens': 500}]</span><br></pre></td></tr></table></figure></div>



<p>修改后</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line"></span><br><span class="line">    model_path,   </span><br><span class="line"></span><br><span class="line">    device_map="auto",</span><br><span class="line"></span><br><span class="line">    torch_dtype=torch.bfloat16,</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure></div>



<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:06&lt;00:00,  2.21s/it]</span><br><span class="line">You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers</span><br><span class="line">Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:06&lt;00:00,  2.13s/it]</span><br><span class="line">You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</span><br><span class="line">tokens/sec: 12.0, time 78.71099257469177, total tokens 1000, total prompts 10</span><br><span class="line">device_map is  {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}</span><br><span class="line">[{'outputs': ['\nThe King is dead. Long live the Queen.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\nThe King is dead. Long live the King.\n', 'They were sent to the country to stay with their eccentric uncle, who lived in a large house that had been in his family for hundreds of years.\nTheir uncle was a very strange man. He was tall and thin and had a long, hooked nose. He was always dressed in a long black cloak, and he had a long, white beard. He was very fond of children, and he was always telling them stories about the witches who lived in the', 'The universe was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. And God said, "Let there be light," and there was light. And God saw the light, that it was good; and God divided the light from the darkness. And God called the light Day, and the darkness he called Night. And the evening and the morning were the first day.\nAnd God said, "Let there', '\nWinston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a colored poster, too large for indoor display, had been tacked to the wall. It', '\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters.\n"It is a truth universally acknowledged, that a single man in possession of a good'], 'num_tokens': 500}]</span><br></pre></td></tr></table></figure></div>


        </div>

        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/Artificial-Intelligence/">#Artificial Intelligence</a>&nbsp;
                    </li>
                
            </ul>
        

        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                        rel="prev"
                        href="/2024/03/29/nlp-6/"
                        >
                            <span class="left arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-left"></i>
                            </span>
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">【学习笔记】自回归模型和GPT</span>
                                <span class="post-nav-item">Prev posts</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                        rel="next"
                        href="/2024/03/27/llm-2/"
                        >
                            <span class="title flex justify-center items-center">
                                <span class="post-nav-title-item">【学习笔记】大模型训练：数据并行</span>
                                <span class="post-nav-item">Next posts</span>
                            </span>
                            <span class="right arrow-icon flex justify-center items-center">
                                <i class="fa-solid fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        


        
    </div>

    
        <div class="toc-content-container">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">llama2部署记录</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#VLLM%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95"><span class="nav-text">VLLM部署记录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chat-completion%E5%8A%9F%E8%83%BD"><span class="nav-text">chat completion功能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Accelerate%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95"><span class="nav-text">Accelerate部署记录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#demo-1-%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8"><span class="nav-text">demo 1 简单使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-demo"><span class="nav-text">llm demo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llm-%E6%89%B9%E5%A4%84%E7%90%86-demo"><span class="nav-text">llm 批处理 demo</span></a></li></ol>

    </div>
</div>
        </div>
    
</div>



                

            </div>

            

        </div>

        <div class="main-content-footer">
            <footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2023</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Julian</a>
        </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.4.4</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
            
                
        
                
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex justify-center items-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>


    
<script src="/js/libs/Swup.min.js"></script>

<script src="/js/libs/SwupSlideTheme.min.js"></script>

<script src="/js/libs/SwupScriptsPlugin.min.js"></script>

<script src="/js/libs/SwupProgressPlugin.min.js"></script>

<script src="/js/libs/SwupScrollPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
        ],
        containers: ["#swup"],
    });

    swup.hooks.on("page:view", () => {
        Global.refresh();
    });

    // if (document.readyState === "complete") {
    //
    // } else {
    //     document.addEventListener("DOMContentLoaded", () => init());
    // }
</script>






<script src="/js/utils.js" type="module"></script>

<script src="/js/main.js" type="module"></script>

<script src="/js/layouts/navbarShrink.js" type="module"></script>

<script src="/js/tools/scrollTopBottom.js" type="module"></script>

<script src="/js/tools/lightDarkSwitch.js" type="module"></script>

<script src="/js/layouts/categoryList.js" type="module"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts" data-swup-reload-script>
    
        
<script src="/js/libs/anime.min.js"></script>

        
<script src="/js/tools/tocToggle.js" type="module"></script>

<script src="/js/layouts/toc.js" type="module"></script>

<script src="/js/plugins/tabs.js" type="module"></script>

    
</div>


</body>
</html>
